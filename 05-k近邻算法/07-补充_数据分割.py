import numpy as np
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import KFold, StratifiedKFold

"""
    通常我们假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥。
    原因举例：
    老师出了10道习题供同学们练习，考试时老师又用同样的这10道题作为试题，这个考试成绩能否有效反映出同学们学得好不好呢？
    答案是否定的，可能有的同学只会做这10道题却能得高分。
    我们希望得到泛化性能强的模型，好比是希望同学们对课程学得很好、获得了对所学知识“举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。
    显然，若测试样本被用作训练了，则得到的将是过于“乐观”的估计结果。

    对一个数据集既要训练也要测试的做法：
    1 留出法
    2 交叉验证法
    3 自助法

    1 留出法
    将数据集分为两部分，一部分训练，一部分测试，但要保证数据分布的一致性。
    可以采用分层采样。
    单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，
    一般要采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。
    训练集和测试机占比会影响模型。
    训练集过多，评估结果可能不够稳定准确。
    测试集过多，被评估的模型与训练出的模型相比可能有交大差别，从而降低评估结果的保真性
    该问题没有完美的解决方案，常见做法，2/3~3/4用于训练，剩余用于测试
    留出法api:sklearn.model_selection.tranin_test_split(x,y,test_size=测试占比，random_state=随机种子)
    留一法：每次只抽取一个样本作为测试集，不受随机样本分布方式的影响。
    留一法api:sklearn.model_selection.LeaveOut()  该对象.split(data)

    留一法优缺点：
    优点：
        留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在绝大多数情况下，
        留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。因此，留一法的评估结果往往被认为比较准确。
    缺点：
        留一法也有其缺陷:在数据集比较大时，训练m个模型的计算开销可能是难以忍受的(例如数据集包含1百万个样本，
        则需训练1百万个模型，而这还是在未考虑算法调参的情况下。

    2. 交叉验证
    将训练集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从D中通过分层抽样得到。
    然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为验证集；这样就可获得k组训练/验证集，
    从而可进行k次训练和验证，最终返回的是这k个验证结果的均值。
    除了GridSearchCV之外，还有KFold（）, StratifiedKFold


    3. 自助法(有放回的采样，测试集与训练集非互斥)
    我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，
    这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。
    “自助法”( bootstrapping)是一个比较好的解决方案，它直接以自助采样法( bootstrap sampling)为基础。给定包含m个样本的数据集D，
    我们对它进行采样产生数据集D:

    每次随机从D中挑选一个样本，将其拷贝放入D，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被到；
    这个过程重复执行m次后，我们就得到了包含m个样本的数据集D′，这就是自助采样的结果。
    显然，D中有一部分样本会在D′中多次出现，而另一部分样本不出现。可以做一个简单的估计，
    样本在m次采样中始终不被采到的概率是(1-1/m)^m取极限得到0.368
    通过自助采样，初始数据集D中约有36.8%的样本未出现在采样数据集D′中。

    于是我们可将D′用作训练集，D\D′用作测试集；这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、
    没在训练集中出现的样本用于测试。
    这样的测试结果，亦称“包外估计”(out- of-bagestimate）
    自助法优缺点：
    优点：
        自助法在数据集较小、难以有效划分训练/测试集时很有用；
        此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大的好处。
    缺点：
        自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。因此，在初始数据量足够时；留出法和交叉验证法更常用一些。
"""


def main():
    """
    supplement_data_segmentation
    补充       数据    分割
    :return:
    """
    # 1 留出法：留一法
    data = [1, 2, 3, 4]
    loo = LeaveOneOut()
    for train, test in loo.split(data):
        print("%s %s" % (train, test))
    print("--------------------------------")
    # 2 交叉验证：KFold和StratifiedKFold
    x = np.array([
        [1, 2, 3, 4],
        [11, 12, 13, 14],
        [21, 22, 23, 24],
        [31, 32, 33, 34],
        [41, 42, 43, 44],
        [51, 52, 53, 54],
        [61, 62, 63, 64],
        [71, 72, 73, 74]
    ])  # 0  1  2  3  4  5  6   7
    y = np.array([1, 1, 1, 1, 1, 1, 0, 0])
    """
    用法：
    将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，
    得到n_splits个结果
    StratifiedKFold的用法和KFold的区别是：SKFold是分层采样，确保训练集，测试集中，各类别样本的比例是和原始数据集中的一致。
    注意点：
    对于不能均等分数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，
    其余子集都只有n_samples // n_splits样本
    参数说明：
    n_splits：表示划分几等份
    shuffle：在每次划分时，是否进行洗牌
    ①若为False时，其效果等同于random_state等于整数，每次划分的结果相同,random_state不生效，新版本设置random_state会报错
    ②若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的,
    属性：
    ①split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器
    """
    kfolder = KFold(n_splits=4, random_state=0, shuffle=True)
    sfolder = StratifiedKFold(n_splits=4, random_state=0, shuffle=True)
    # KFold:
    for train, test in kfolder.split(x, y):
        print("train: %s, test：%s" % (train, test))
    print()
    # StratifiedKFold:各类别样本的比例是和原始数据集中的一致。
    for train, test in sfolder.split(x, y):
        print("train: %s, test：%s" % (train, test))


if __name__ == '__main__':
    main()
